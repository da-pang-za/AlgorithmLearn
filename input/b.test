基于异质多模态信息的序列推荐算法的设计与实现

摘  要

基于异质多模态的序列推荐算法是现代推荐算法中的一个研究方向，它根据用户的历史购买序列以及额外的异质信息动态地学习用户变化的兴趣，能够及时捕捉用户偏好，解决商品冷启动和数据稀疏的问题，从而进行下一次精准的推荐。
目前的研究中，多模态推荐算法和序列推荐算法都有着各自的发展。多模态推荐算法考虑了用户历史购买商品的多种模态特征，以用户的历史购买记录为依据，但大都对于时序考虑不周，没有考虑用户变化的兴趣。为了动态地捕获用户的兴趣，基于序列的推荐算法得到迅速发展，如早期的基于马尔科夫链（Markov Chain，MC）的模型和现代的基于RNN的模型，以及最近提出的基于attention的模型。其中，基于attention机制的推荐模型能够动态的捕获用户的长短期偏好，在许多数据集上取得了state-of-the-art的表现。
然而，现有的基于attention的模型只考虑了用户的商品购买序列信息，在譬如电商推荐领域，许多商品信息（如图片等）以及用户反馈信息（如评论等）都可能反应出用户的兴趣和偏好，所以本文提出了一种融合视觉和文本信息的基于注意力机制的序列推荐算法（Multimodal Self-Attentive Sequential Recommendation，MSASR），为原本基于商品购买序列的推荐模型融入了额外的多模态信息，提高了商品推荐的准确度，缓解了商品冷启动和数据稀疏的问题。模型在真实世界的三个数据集进行验证，实验数据表明该模型可以取得良好的表现。

关键字 多模态 序列推荐 注意力机制

















Design and Implementation of Sequential Recommendation Algorithm Based on Heterogeneous Multimodal Information

ABSTRACT

Sequential recommendation algorithm based on heterogeneous multimodality is a research direction in modern recommendation algorithms, which dynamically learns users' changing interests based on their historical purchase sequences and additional heterogeneous information, and is able to capture user preferences in time to solve the problems of cold start and data sparsity of goods for the next accurate recommendation.
Both multimodal recommendation algorithms and sequential recommendation algorithms have their own development in the current research. Multimodal recommendation algorithms consider multiple modal characteristics of users' historical purchase of goods, based on users' historical purchase records, but most of them are not well considered for temporal order and do not consider users' changing interests. In order to capture users' interests dynamically, sequence-based recommendation algorithms have been rapidly developed, such as the early Markov Chain (MC)-based models and modern RNN-based models, as well as the recently proposed attention-based models. Among them, the recommendation models based on the attention mechanism can dynamically capture users' long- and short-term preferences and achieve state-of-the-art performance on many datasets.
However, the existing attention-based models only consider users' product purchase sequence information. In the field of e-commerce recommendation, for example, many product information (e.g., images, etc.) and user feedback (e.g., reviews, etc.) may reflect users' interests and preferences, so this paper proposes an attention-based sequential recommendation algorithm that incorporates visual and textual information named Multimodal Self-Attentive Sequential Recommendation (MSASR), which incorporates additional multimodal information to the original recommendation model based on product purchase sequences, improves the accuracy of product recommendations, and alleviates the problems of product cold starts and data sparsity. The model is validated on three real-world datasets, and the experimental data show that the model can achieve good performance.

KEY WORDS  multimodal  sequential recommendation  attention

	目  录


第1章 引言	1
1.1 研究背景及意义	1
1.2 课题任务	1
1.3 论文组织结构	2
第2章 相关技术介绍	3
2.1 传统推荐算法	3
2.2 基于深度学习的推荐算法	4
2.2.1 序列推荐算法	5
2.2.2 多模态推荐算法	6
2.3 本章小结	6
第3章 设计与实现	7
4.1 改进模型的动机	7
4.2 模型相关概念	8
4.3 模型实现细节	9
4.3.1 Embedding层（嵌入层）	9
4.3.2 attention层（注意力层）	10
4.3.3 Feed-Forward层（前馈神经网络层）	11
4.3.4 Add&Norm层（残差和正则层）	12
4.3.5 Text&Image层（图像文本处理层）	12
4.3.6 Dropout层	13
4.3.7 cross attention层（交叉注意力层）	14
4.3.8 Prediction层（预测层）	14
4.4 本章小结	14
第4章 实验及实验结果	15
4.1 运行环境及框架	15
4.2 数据集	15
4.3 评价标准	16
4.4 Baseline	17
4.5 损失函数和优化器	18
4.6 模型参数设置	18
4.7 与其他模型的比较	18
4.8 剥离实验	20
4.8.1 对冷启动的问题的影响	20
4.8.2 不同隐层大小对实验结果的影响	20
4.8.3 不同模块对实验结果的影响	21
4.9 本章小结	22
第5章 总结与展望	23
5.1 论文工作总结	23
5.2 后续工作展望	23
参考文献	24
致 谢	26


第1章 引言
1.1 研究背景及意义
近年来，随着互联网的发展，对于序列信息的应用越来越广泛，如何合理的利用这些信息，准确地预测用户的下一行为，提升用户的体验，是序列信息应用的一个研究方向。
在电商平台，一位用户购买了某些商品以后，系统会给出用户一张生成的推荐列表，其中包含用户可能在未来购买的商品。更进一步，我们可以充分利用用户的历史购买记录，生成一个以时间为序的购买序列，从中学习用户的偏好迁移，捕捉用户变化的兴趣。然而，对于新商品或者冷门的商品，由于购买行为的稀疏性，用户与商品的交互行为较少，不能有效地利用这些序列信息学习到商品之间的关联性，从而导致商品冷启动问题。一个解决方案是融入更多的额外信息作为序列的补充。例如，每个商品都有许多固有的信息（如图片等）以及用户反馈信息（如评论等），如何将这些异质信息综合作用于序列推荐算法，从而能够捕捉用户偏好，是异质多模态序列推荐算法的核心问题。
可以想象，在实际生活中，用户的兴趣爱好并不是仅可以通过商品序列获得，商品的图片信息和用户的评论信息都可以很好地反应出用户的偏好。举个例子来说，一位女士某一天在某购物网站上购买了浅绿色的衬衫并且给出了好评，且在不久后她又购买了一条绿色的裙子，那么我们可以推测这位女士大概率比较偏爱绿色系小清新的风格，也许在下一次她就会选择一顶小清新的帽子，我们就可以以此做出推荐。所以可见充分利用这些额外信息到序列推荐算法中可以提高推荐的准确度，解决商品推荐冷启动的问题。
1.2 课题任务
我们的任务就是构建一个考虑多种异质信息的序列推荐模型，缓解商品冷启动问题，提高推荐的准确度。异质多模态序列推荐算法的目标是根据用户的历史购买序列的上下文信息以及图像和文本等额外信息学习到用户的兴趣和偏好，从而推荐用户下一件可能购买的商品。该研究方向的最大挑战是如何在给定的用户历史购买序列中寻找商品之间的关联信息以及如何利用图像和文本等异质信息，从而高效地学习到用户的偏好，进行下一次购买的推荐。
考虑如何在已有的序列推荐算法上加入异质多模态的信息，缓解商品的冷启动问题，受[1][2]模型的启发，我们提出一种异质多模态融合的基于注意力机制的推荐模型（Multimodal Self-Attentive Sequential Recommendation，MSASR），该模型融合了商品的图片和用户的评论信息，加强了模型的表达能力，能根据用户的历史购买商品序列以及其相应的图片信息和评论信息来进行下一次购买的推荐。首先，我们分别获取了商品图片和用户评论的视觉和文本特征并加以融合，我们认为这些特征可以充分反映用户的偏好；接着，我们把序列中的每一个商品表示成一个稠密向量；然后，我们把融合特征和商品向量的序列作为模型的输入；最后根据输出的稠密向量做预测。具体来说，我们做了如下几点工作：
首先，我们采用CNN的方式提取图片特征，具体来说为把商品图片通过预训练好的ResNet18，提取图片的特征，接着，对评论做Latent Dirichlet Allocation（LDA）分析，获取评论的特征，并将其两种特征融合后输入到一个attention层用于捕获图片和评论融合特征中的上下文信息。
然后，将某用户的商品购买序列（ID序列）做Embedding，接着输入一个attention层，用于捕获商品序列中的上下文信息，让模型能够学习到商品序列中的关联性信息。
最后，将两个上下文信息通过第三个attention层把两种信息融合起来，做最后的输出，根据获得的Embedding向量做预测。
1.3 论文组织结构
论文的剩下篇幅按以下逻辑组织：
第2章：相关技术介绍。在这一章我们将回顾推荐系统的发展历程，介绍了一些传统推荐算法和深度学习推荐算法的相关工作，以及深度学习推荐系统中的两个方向——序列推荐算法和多模态推荐算法的相关成果。
第3章：设计与实现。在这一章我们将介绍整体模型，接着介绍我们改进模型的动机，然后依次介绍每个模块的实现细节以及相关公式。
第4章：实验及实验结果。介绍了验证模型所采用的数据集、评价指标、Baseline、训练的损失函数和优化器、参数设置以及实验结果。其中实验结果包括了我们的模型与其他模型的对比以及额外的三个实验：分别是验证模型对冷启动问题的影响，不同隐层大小对实验结果的影响，以及不同模块对实验结果的影响。
第5章：总结与展望。对本次毕业设计的完成过程进行了总结，并且对后续可能的工作做出了展望。










第2章 相关技术介绍
推荐算法是根据用户的历史购买记录以及其他的一些上下文信息建模用户偏好，从而为其进行推荐的算法，有着悠久的发展历史。在这一章节，我们将从传统推荐算法出发，再到基于深度学习的推荐算法，并介绍其中的两个研究方向：序列推荐算法和多模态推荐算法。
2.1 传统推荐算法
传统推荐模型凭借其可解释性强，硬件要求低和易于训练部署的优势，在工业界仍占有着一席之地，可以说，传统推荐模型奠定了深度学习推荐模型的基础。
经典协同过滤模型[3]（Collaborative Filtering，CF）可以认为是推荐算法的起点，它从用户相似度和商品相似度出发，构建用户-商品共现矩阵，即将用户和商品的交互信息组成一个矩阵，矩阵中的有效值表明某位用户曾经购买过某件商品，基于统计的方法给用户推荐商品，并衍生出了商品协调过滤（ItemCF）和用户协同过滤（UserCF）两种算法，分别根据商品相似度和用户相似度为用户推荐商品。但对于大量商品和用户的实际情况，CF模型存在数据稀疏的问题。
为了增强模型的表达能力，提高模型处理稀疏数据的能力，从CF模型衍生出矩阵分解模型[4]（Matrix Factorization，MF），该模型将协同过滤中的共现矩阵用矩阵分解的方式分解为用户矩阵和商品矩阵，利用两者的隐向量做内积预测用户购买某种商品的概率，从而进行推荐。
以上的两种模型仅仅考虑了用户和商品的反馈信息，然而，现实世界有许多商品信息也可被用于推荐，于是后来又发展出了融合其他商品特征的回归模型。逻辑回归作为机器学习的基础模型，有着简单、易用的特点，但其局限性也十分明显：表达能力依然不足，无法进行特征交叉、特征筛选等一系列较为高级的操作，由此不可避免地造成信息的损失。在利用单一特征而非交叉特征进行判断的情况下，有时甚至会得出错误的结论，“辛普森悖论”就是典型的例子。
针对特征交叉的问题，算法工程师提出了因子分解机模型[5]（Factorization Machine，FM），具体来说，FM模型为每个特征学习一个隐权重向量。在进行特征交叉时，用两个特征的隐向量的内积作为交叉特征的权重。本质上，FM模型引入隐向量的做法和MF模型用隐向量代表用户和商品的方法有异曲同工之妙，可以说，FM模型将矩阵分解隐向量的思想做了扩展，从当初的用户和商品隐向量扩展到了所有特征上。FFM模型[6]在FM模型的基础上进一步提出了特征域交叉的方式，用二阶特征交叉的方法增强了模型的表达能力。Facebook在其基础上提出了GBDT+LR模型[7]，实现了更高维度的特征交叉，同时也第一次完成了用单独的模型实现端到端的特征工程。阿里巴巴提出了LS-PLM模型[8]，作为逻辑回归的自然推广，具有了与深层神经网络极其相似的结构，被应用于实际的推荐系统中。
上述提到的这些模型奠定了推荐系统的基础，推荐算法也由此跨入了深度学习时代。
2.2 基于深度学习的推荐算法
与传统推荐模型相比，一方面，深度学习模型往往有着更强的表达能力和泛化能力，能够挖掘用户深层的偏好，另一方面，深度学习的模型结构非常灵活，能够根据业务场景和数据特点灵活地调整模型结构，使得模型与应用场景完美契合，因此成为研究的主流。
AutoRec模型[9]作为神经网络在推荐系统中的初步尝试，用单层的神经网络为用户做推荐，把自编码器的思想引入推荐系统，输入商品的评分向量，输出所有用户对商品的评分预测。
Microsoft提出了Deep crossing模型[10]，相比于简单的AutoRec模型表达能力不强的问题，该模型完整地解决了从特征工程、稀疏向量稠密化、多层神经网络进行优化拟合等一系列深度学习在推荐系统中应用的问题。模型采用Embedding+多层神经网络的经典神经网络结构，把所有原始特征经Embedding后输入神经网络层，将全部特征交给模型处理，相比上面提到的传统推荐算法中提到的FM和FFM模型只具备二阶特征交叉的能力，Deep crossing为深度模型融入了多种特征，增强了模型的表达能力。Deep crossing模型可以调整模型的深度进行特征的深度交叉，这也是该模型名字的由来。
NeuralCF模型[11]把传统的矩阵分解中的用户向量和商品向量的点积操作换成由神经网络代替的互操作，又可被认为是广义的矩阵分解模型，同样的，NeuralCF模型也是基于CF的模型，没有考虑其他类别的特征。
PNN模型[12]针对不同特征域之间的交叉操作，定义了“内积”，“外积”等多种操作，在深度学习框架上提高模型对特征的交叉能力。
Google提出了Wide&Deep模型[13]，提供了融合不同网络的新思路，在业界中发挥了重大影响力。其中Wide部分通过交叉积变换，加大了某些特征对结果的影响，增强了模型的记忆能力，Deep部分采用深度神经网络处理特征，增加了模型的泛化能力，这样的结构特点使得模型兼具了逻辑回归和神经网络的优点：能够迅速处理并记忆大量历史行为特征，并且具有强大的表达能力。后来又有人在此基础上提出了Deep&Corss模型[14]，用cross网络替代Wide&Deep模型中的Wide部分，解决了Wide&Deep模型人工组合特征的问题。FNN模型[15]用FM的参数来初始化神经网络的Embedding层参数，加快网络的收敛速度。DeepFM模型[16]在Wide&Deep的基础上，用FM替代了Wide部分，加强了Wide部分特征交叉的能力。NFM模型[17]用神经网络替代了FM中二阶隐向量交叉的操作。
特征工程自动化的道路到这里几乎已经穷尽了所有的可能，该类模型进一步提升的空间已经非常小了，在此之后，越来越多的深度推荐模型开始探索结构上的测试，诸如注意力机制、强化学习、序列模型等在其他领域大放异彩的模型也逐渐进入到了深度学习领域，并且在推荐模型上提升明显。
AFM模型[18]在FM的基础上，为二阶隐向量的交叉加入了注意力得分，并使用注意力网络自动学习注意力得分。DIN模型在深度学习推荐模型的基础上，引入了注意力机制，利用用户历史商品和目标广告商品的相关性计算注意力得分，从而进行推荐。DRN模型[19]将强化学习同于推荐，实时学习和更新参数，快速捕捉用户兴趣。
可见，基于深度学习的推荐算法有着各种而已的研究方向，其中，序列模型能够根据用户的历史购买序列动态学习用户偏好，为用户推荐商品，而额外的多模态信息将强化模型的表达能力。下面将介绍深度学习中的序列推荐算法和多模态推荐算法。
2.2.1 序列推荐算法
为了学习用户的动态兴趣变化趋势，序列推荐算法成为了深度学习中的一个热门探究方向，它根据用户的历史购买序列做推荐，可以快速捕捉用户随时可能变化的偏好，具有实际意义。
序列推荐算法主要可以分为基于MC的序列推荐模型和基于RNN的序列推荐模型，以及基于attention的模型。
不同于传统的基于CF的模型，早期的序列推荐模型更多的是基于马尔可夫链（Markov Chain，MC）。许多的序列模型通过建模商品和商品之间的转移矩阵来从连续的商品序列中想学习到用户的兴趣。FPMC模型[20]融合了MF模型和转移矩阵来获取用户偏好，其中的转移矩阵就是一个MC。基于MC的模型基于这样一个假设，用户的下一次购买行为只与用户的上一次或者上几次的行为有关，可以学习到用户的短期偏好，所以在稀疏的数据集上能取得不错的表现。与MC模型类似的还有基于CNN的模型，例如Caser模型[21]把商品序列转化成二维矩阵并在上面做卷积去建模用户偏好。
与基于MC的模型不同，基于RNN的模型根据用户和商品的历史交互序列来学习用户兴趣。RNN凭借其独特的卷积结构，它能够很好地捕捉时序依赖，为用户推荐商品。然而，RNN也存在梯度消失和梯度爆炸的问题，人们又在其基础上进行了许多探索和改进，最终门激活函数取得了成功，由此发展出了LSTM和GRU的结构。 GRU4Rec模型[22]可以被认为是早期把RNN用于序列推荐的模型，他用GRU单元解决了长序列学习中的梯度消失和梯度爆炸问题，可以学习到用户的长期偏好，但由于是基于长序列的学习，需要稠密的数据集，且由于RNN的结构特点，某一时刻的输出依赖于当前时刻的输入和上一时刻的输出，不能实现并行的运算，存在计算效率低的问题。
近年来，attention机制在图片描述和机器翻译中取得了很好的效果。该机制的核心思想是认为序列的输出取决于输入的某些相关部分，并且那些相关部分能够被机器自动学习到。attention机制的另一个好处是比较传统机器学习的模型有更多的可解释性。传统的attention模型往往与其他模型相结合，例如attention+RNN，attention+FM等。最近，一种纯attention实现的序列-序列模型被提出，即Transformer模型[23]，实现了机器翻译任务state-of-the-art的表现，创造性地提出了self-attention模块。BERT4Rec模型[24]采用了双向的注意力网络建模用户的偏好，同样取得了不错的效果。SASRec模型[1]把self-attention用于序列推荐，可以自动学习用户的偏好，且由于self-attention的并行计算能力，模型能够很快地进行训练，取得了良好的效果。
2.2.2 多模态推荐算法
为了缓解推荐系统冷启动和数据稀疏的问题，即新商品因缺少历史交互信息而不能被很好地推荐和用户交互数据较少导致的商品关联性不能被很好学习的问题，目前的一个方向是融合商品的多模态信息。所谓的多模态学习旨在通过机器学习的方法实现理解和处理多源模态信息（例如图像和文本信息等），缩小不同模态之间的异质性差异，从而为上层应用服务。多模态推荐算法即应用多模态信息到推荐系统中，可以粗略地分为基于视觉信息的学习方法，基于文本信息的学习方法以及基于多模态信息的学习方法。
对于视觉信息，传统的MF模型仅仅依赖稀疏的用户反馈矩阵来学习用户偏好忽略了商品本身的特征，因此VPBR模型[25]把图片特征通过CNN进行提取，将图片特征与潜在的特征联合在一起进行推荐。VECF[26]模型更进一步通过应用一个attention层给出了视觉信息的细粒度解释。
对于文本信息，ALFM模型[28]通过融合评论和评分信息，提出了一种评分预测模型，综合了文本信息做推荐。RNS模型[29]把评论信息融入到序列推荐中，提供了融合文本信息到序列模型中的思路。
对于多模态信息，MV-RNN模型[27]融合了商品的图片和描述信息，把这些信息交给GRU和LSTM网络进行推荐，在序列中融入了额外的异质信息，对推荐起到了良好的效果。
2.3 本章小结
在这一章，我们介绍了传统推荐算法和基于深度学习的推荐算法，并介绍其中的两个研究方向：序列推荐算法和多模态推荐算法。其中，我们介绍了序列推荐算法中的不同方向，特别是基于注意力机制的SASRec模型。SASRec模型采用self-attention机制实现了推荐算法较高的推荐指标，为我们的模型所借鉴。









第3章 设计与实现
在这一章节，我们先说明改进模型的动机，接着给模型的一些概念做形式化的定义，最后我们介绍模型的细节部分以及相关的公式。
4.1 改进模型的动机
从序列中建模用户偏好的关键是学习到商品与商品之间的上下文关联性。商品之间的关联性可以有多种方式来获取，例如从商品的属性特征计算商品的相似度等，然而从近期的相关研究中可以看到，商品序列中隐藏的商品之间的关联性在多数情况下能够有更好的表现，即通过不同用户的购买序列可以学习到商品与商品之间的关联性，从而为已知序列的用户推荐下一次购买的商品。
SASRec模型对商品ID序列建模，构建了商品之间的关联性信息，从而学习用户偏好。我们在其基础上进行改进：我们认为，商品的关联性特征虽然可以通过用户商品购买序列来得到很好的体现，但如果加入了更多的高层的关联性特征（这里指的高层的关联性特征指的是由商品图片和用户评论信息学习到的用户偏好），模型应当会有提升。我们认为，诸如商品属性等低层信息的引入可能会为模型制造过多的噪声，因为商品的属性特征错综复杂且无法准确地定义，而图片和评论容易准确获取，且在经过特征提取后会保留一些高层的共性特征，比如商品图片特征可能会反映用户的审美信息，用户评论特征会体现用户对于某类商品的喜爱程度，从而降低噪声商品对结果的影响，故我们在SASRec的基础上融入了额外的多模态信息。
在具体的实施中，我们提取了商品图片和用户评论的特征，并将两种特征加以融合。在把这些特征加入到模型的过程中，我们考虑了两种基本模型，第一种模型是把这些融合特征与商品序列一起从同一个attention层输入到模型中，但实验结果较SASRec反而有所降低，故我们采取了第二种方式：将融合特征通过另一个attention层，得到一个输出，我们认为这个输出是从商品图片和用户评论中学习到了用户审美偏好的隐向量，然后我们以这个隐向量作为第三个attention层的query向量，以商品序列通过attention层的输出作为key和value，将三者输入到第三个attention层中，融合两种信息做推荐。这里以商品序列作为value的原因是我们最终输出是商品的向量表示，而以多模态特征作为query和以商品序列作为key的原因是，在同一时间，query向之前的所有商品的key做查询，相当于将这个额外信息与之前的所有商品综合考虑，我们认为这种方法可以很好地融合额外信息，且这些额外信息能够帮助模型更快地收敛，实验结果也表明我们的模型确实比SASRec有所提升。
4.2 模型相关概念
表4-1 模型相关的一些概念
概念	描述
	用户集
	商品集
	商品图片集
	用户评论集
	商品的维特征
	图片的维特征
	评论的维特征
	图片和评论的维融合特征
	用户的购买商品及其特征序列：
相关概念的描述如表4-1所示，我们的任务就是从用户的历史交互序列学习到用户的兴趣偏好，从而进行下一次购买商品的推荐。出于这个目的，我们做了如上定义，和是用户集和商品集，对于模态信息，我们用和来定义图片集和评论集，对于商品，我们用一个稠密向量表示，对于图像和文本，我们提取其特征，用向量和表示，是和的融合特征。基于这些概念，我们考虑用户购买的商品序列以及相关的多模态特征序列，作为模型的输入，从而给出用户下一个可能购买的商品，作为模型的输出，任务的形式化定义如下：

即给定时刻前的商品和融合特征序列信息，输出时刻的商品。
4.3 模型实现细节

图 4-1 MSASR模型概览
我们的模型（Multimodal Self-Attentive Sequential Recommendation，MSASR）如图4-1所示。
模型主要包括三部分：（1）为商品ID交互部分（见图左下部分），其输入为通过了Embedding层的商品ID序列，用于学习商品之间的关联信息。（2）为多模态特征部分（见图右下部分），其输入是提取到的商品图片和用户评论的融合特征，用于学习融合特征之间的关联信息。（3）为信息融合部分（见图上半部分），把学习到的多模态特征加入到商品序列的输出中，让模型能够考虑到多模态的信息，做最终的推荐。下面我们就这三部分的具体模块加以介绍。
4.3.1 Embedding层（嵌入层）
把“对象”表示成能被神经网络学习的输入，往往需要对其进行编码。最简单的编码方式就是把对象进行one-hot编码，即对于一个类别数目为的对象，我们用一个维向量表示，其中属于某一种类某个位置就为1，其它都为0。然而，该编码方式存在很多缺陷，例如当列别数目很大时，向量长度会变得非常大，容易造成向量稀疏的问题，另外，one-hot编码要求每个类别之间相互独立，如果之间存在某种连续型的关系，就应该使用分布式的编码方式。
Embedding技术就是一种典型的分布式编码方式，在深度学习中，特别是在以推荐、广告、搜索为核心的互联网领域中发挥着重大作用，是深度学习推荐系统的“基础和核心操作”。通俗来讲，Embedding就是把一个“对象”表示成一个低维稠密的向量，以便于上层神经网络处理。具体来说，Embedding是根据序列中的上下文信息构建的对象的中间表示，可以理解为把神经网络某一层的权重作为对象的向量表示。
在我们的模型中，对于一个给定的商品ID序列中的每个商品ID，我们让其通过Embedding层，实际上就是一个全连接网络，把商品ID表示成一个低维稠密向量的形式。另外，对于一个具体的神经网络而言，输入的维度必须是相同的，所以我们把序列填充或者截断成相同长度的序列，其中为序列的固定长度。如果序列长度超过，我们就把它截断为，如果序列长度不足，我们就用一个填充项填充到。
我们通过商品的Embedding获得了一个Embedding矩阵，其中为商品ID通过Embedding后的向量长度，且的某一行就是对应的商品ID的Embedding向量。那么对于固定了长度的输入序列，我们可以的到其向量表示。到此，我们就获得了商品ID序列的向量化表示。
4.3.2 Attention层（注意力层）
attention机制有着悠久的发展历史，在NLP领域被广泛应用。2017年，谷歌翻译团队采用了纯attention机制的Transformer模型[23]替代了传统的CNN和RNN模型，在翻译领域取得了重大突破，随之该模型也在深度学习的各领域大放光彩。通俗来讲，attention模型用于计算序列的“相关性”，对序列中不同位置的重要信息加以关注并重点学习，挖掘序列中的关联性特征，其输入是一个序列，输出另一个序列，是一种序列到序列的模型。具体来说，典型的attention模型的核心思想就把输入的向量序列分别乘上三个不同权重的矩阵表示为query Q，key K和value V，然后用query和key计算注意力权重，然后把权重通过一层softmax层做归一化，最后用这个权重与value相乘作为输出。权重的计算方式多种多样，这里我们采用了内积的计算方式，我们用key和query做内积作为输入的权重，然后计算与value的加权和，优点是简单快捷，可以降低模型的复杂程度。用公式表示为：

其中的作用是防止内积和过大。
另外论文[23]中还提到了Multi-Head attention机制，称该机制能有效地提升模型的表达能力。该机制指的是在上述模型的基础上把输入序列同时乘上多个不同的矩阵得到不同的query Q，key K和value V，计算权重后通过softmax层做输出，然后把输出的向量做拼接，在乘上一个矩阵使得输出和输入的向量长度相同。


我们后面的实验会对比采用了Multi-Head attention机制和没有采用该机制的模型进行对比，详情见实验部分。
我们认为attention机制可以根据输入和输出序列的关联性特征，自动为输入序列的不同位置赋予不同的权重，并由此产生正确的输出。另外，由于该机制的特殊结构，我们可以实现计算的并行性，大大增加了计算速度。而且，该模型可以考虑到整个序列，克服了CNN卷积核大小固定的问题，同时也解决了RNN梯度消失和梯度爆炸的问题。可以认为，attention是卷积核无限大的CNN和可以实现并行运算的RNN。
另外重要的一点是，上述模型还没有加入位置信息，即模型对于早期的信息和最近的信息考虑的权重是一样的，然而对于序列而言不同时间的信息权重是不同的，所以典型的attention模型还需要加入一个位置信息。
在我们的模型中，对于输入向量，我们给他赋予一个位置信息，（其中可以将一个递增序列通过Embedding获得），然后将乘以三个矩阵、、得到相应的、、矩阵，最后计算输出：

在加入位置信息以后，attention机制就可以自动的考虑不同时期的输入对于当前输出的影响，从而动态调整不同位置的权重，做更加合理的输出，在我们的模型中就可以及时捕捉用户变化的近期兴趣。
值得注意的一点是，上述模型会考虑到输入的整个序列，然而在序列推荐算法中，我们往往只会关注当前时刻之前的序列信息，所以在进行上述计算时往往需要加入一个mask屏蔽当前时刻后面的序列。
4.3.3 Feed-Forward层（前馈神经网络层）
由现实生活中的经验可以判断，一个函数的输入和输出往往不会是线性的关系，神经网络中激活函数的作用就是为了学习到这些非线性的特征，从而更好地拟合所给的数据，故神经网络可以通过增加深度来学习跟高维度的特征。
由上述介绍可以看出，虽然attention层可以自动学习到序列中的相关权重并实现并行运算，但单纯的矩阵相乘仍然是属于线性计算，只能学习到线性特征，对于一个复杂的问题而言，往往还需要学习一些非线性的特征，所以通常会在attention层上加入一个全连接网络，并通过激活函数，学习到这些非线性特征。
在我们的模型中，我们把上一层的输出通过一个一维卷积网络，其本质上是共享了部分参数的全连接网络，然后通过ReLU函数，再通过一个一维卷积网络，学习非线性特征：

其中和是的矩阵，和是维向量。
4.3.4 Add&Norm层（残差和正则层）
Normalization用于归一化输入，例如使输入的均值变为0，方差变为1，可以加速梯度下降的求解速度，增强模型的泛化能力。Normalization的方式有很多，这里我们使用的是Layer Normalization，相比考虑整个Batch的均值和方差的Batch Normalization，前者不会被同一Batch的其他变量所影响。Layer Normalization的定义如下：

其中为Hadamard product运算，即把两个向量逐元素相乘。和分别为均值和方差，和时学习到的两个值。
随着网络的加深，模型可以学习到更多的特征，然而随之而来的会出现训练时间增长以及梯度消失和梯度爆炸问题。上述的归一化操作可以在很大程度上解决这个问题，使深层神经网络可以在反向传播的随机梯度下降上能够收敛，然而，训练深层的神经网络时往往会出现退化问题，即随着网络深度的增加，模型的准确率会下降。残差神经网络的出现缓解了这个问题，它把低层的特征直接连接到高层，使模型能够直接使用低层的有用的特征，其方法是把低层特征直接与高层特征相加。

其中是直接映射，即没有通过处理的特征，表示残差部分，即通过了处理的特征，直接映射可以让模型直接读取到低层特征，使模型有更好的表达能力。
在我们的模型中，我们在attention层和Feed-Forward层的输出后都增加了Add&Norm层，目的就是加快模型的训练速度，防止模型过拟合。
4.3.5 Text&Image层（图像文本处理层）
图像和文本属于不同模态的特征，我们要从商品图片和用户评论和这两个对象中提取信息，将其融合后处理成神经网络可以处理的向量序列形式。
对于图片信息而言，一般采用CNN对图像做卷积，让网络自动保存自己认为对输出有用的部分。对于商品中没有很好的办法指定标签的图片而言，可以采用在大数据集上预训练好的模型提取图像信息。Residual Network在2015年的ImageNet视觉识别竞赛中取得了压倒性的优势，凭借其容易优化，而且能够通过增加网络的深度来获得更好的效果，它的思想被借鉴到各领域，其内部的残差块使用了跳跃连接，缓解了在深度神经网络中增加深度带来的梯度消失问题。
在我们的模型中，我们采用了Pytorch中预训练好的ResNet18模型，这是一个18层的Residual Network，属于典型的深层卷积残差网络，可将图片分为1000类，输出为1000维向量，每一维表示可能的类别概率，可以用其作为商品图片的特征：

其中代表图像的特征向量。
对于评论信息而言，一般可以采用LDA（Latent Dirichlet Allocation）分析。LDA是一种非监督的学习，该模型可以用来识别文本中隐藏的主题信息，是一种主题生成模型，即我们认为一段句子中的每个词都是以一定概率选择了某个主题，并从主题中以一定概率选择了某个词这样一个过程得到的，属于典型的词袋模型，即不考虑词语的顺序问题。
在我们的模型中，我们先把词转化为词向量，也就是把每个词转化成一个向量的形式，再用采用sklearn中的LatentDirichletAllocation类进行LDA分析，得到一个10维的向量，其中某一维代表该段句子属于某个主题的概率。

其中的代表评论的特征向量。
对于得到的文本和图片信息，我们将其向量进行拼接，得到Text&Image层的输出。

其中的代表融合论的特征向量。
与商品ID的序列处理相似，将对应的商品ID的融合特征向量组成的向量序列通过一个attention层和Add&Norm层，学习用户的兴趣变化。
4.3.6 Dropout层
当输入神经网络的数据量不大而epoch过多时，容易导致过拟合问题，即在训练集上表现良好，在验证集和测试集上表现糟糕。一个可行的方案是加入Dropout层，缓解过拟合的问题。Dropout的原理很简单，就是在训练时，以概率随机丢弃掉一些输入节点，在验证或者测试时又考虑所有节点。
在我们的模型中，我们把输入商品ID的Embedding序列和融合特征的序列通过Dropout，缓解过拟合问题。
4.3.7 Cross Attention层（交叉注意力层）
单模态的attention层通过为不同位置的信息赋予不同的权重，可以学习到模态序列中的同质特征，有些推荐模型为了融合不同的信息采用了cross attention的方式[2]。在一个多模态的cross-attention层中，我们可以综合不同模态的信息，例如，可以根据额外的文本和图像信息推测下一件购买的商品。
在我们的模型中，我们采用异质查询的方式构建模型，我们将从文本图像信息中学到的特征作为query，从商品序列中学到的特征作为key和value，乘以三个矩阵、、得到相应的、、矩阵，最后计算输出，把多模态信息作为序列推荐的补充信息：

其中的表示特征序列向量组成的矩阵，表示通过了Add&Norm层的的输出。
我们认为，这样的方式可以让额外的异质多模态信息作为商品ID序列内在联系的补充，扩展模型的表达能力，为用户做更加精准的推荐。
同样的，将输出的结果通过Add&Norm层和Feed-Forward层做最后的输出。
4.3.8 Prediction层（预测层）
在得到最终时刻的商品ID的输出向量后，我们根据这个向量做预测。由于与输入一样属于同一个向量空间，故该向量与商品ID的Embedding矩阵相乘后所得的向量的每一维的值可以用来表示下一购买商品的可能性，某一维的数字越大代表推荐某个商品的概率越大，表达式如下所示：

其中是对商品在时刻的相关性分数，该分数越大表明时刻推荐商品的概率越高，我们可以就此排序所有的分数为用户生产推荐列表。
4.4 本章小结
在这一章节，我们介绍了我们的模型，模型的细节中我们介绍了不同的模块所产生的作用，特别是对商品序列信息的处理以及对图像和文本序列信息的处理，以及如何将两者融合到模型中共同对模型的输出起作用。





第4章 实验及实验结果
在这一章节，我们将介绍实验运行环境及框架，实验所用到的数据集及相关的处理过程，推荐算法的常用评价指标，与我们模型比较的Baseline模型，训练的损失函数和优化器，实验的参数设置，结果的展示及我们的模型与其他模型的比较分析，以及其他的三个额外实验：对冷启动问题的影响，不同隐层大小对实验结果的影响，不同模块对实验结果的影响。
4.1 运行环境及框架
本项目运行在NVIDIA GeForce RTX 3070上，CUDA版本为11.3，pytorch版本为1.8.1。
4.2 数据集
我们在Amazon公开的数据集（2014版）[30][31]上进行我们的实验，该数据集同时提供商品的元数据，包括商品ID、图片URL等信息，以及评论数据，包括用户ID、商品ID、评论和时间戳等信息，符合我们构建多模态序列推荐算法的需求。具体而言，我们采用了其中的Cell Phones & Accessories，Beauty，和Clothing, Shoes & Jewelry三个数据集，每个数据集都包括了元数据（商品相关数据）和评论数据（用户购买行为相关数据）。
为了获得有效序列并减少模型训练的时间，我们采用以上三个数据集的core-5子集（属于评论数据，元数据仍然采用整个数据集上的数据），所谓core-5子集也就是每个商品至少被5个用户购买，以及每个用户至少购买了5件商品。
我们对数据进行了预处理：我们首先删除了元数据和评论数据中重复的数据和空数据，然后我们求出了元数据和评论数据的交集，从中找出相关的商品的元数据，并按其中的图片URL下载商品图片。最终的数据量如表5-1所示。
表 5-1 预处理后的数据统计
Dataset	#users	#items	#actions	avg act/user	avg act/item
Cell Phones & Accessories	27878	10255	192468	6.9	18.77
Beauty	22363	12076	198064	8.86	16.4
Clothing Shoes & Jewelry	39387	22584	272841	6.92	12.08
对于处理好的用户评论数据，我们按其时间进行排序，并为每个用户构建购买序列，作为我们模型的输入。其中序列的倒数第一个数据作为测试集，倒数第二个数作为验证集。
4.3 评价标准
推荐系统的评价标准各不相同，但通常会为用户返回一个长度为K的列表，按用户可能购买商品的概率排序前K件商品，称为Top-K推荐。我们采用了常用的Top-K推荐标准：Hit Rate@10以及NDCG@10。
Hit Rate@10是指真实的下一次购买的商品在Top-10推荐列表中出现的次数所占的比例，形式化定义为：

其中分母为所有的测试集合的大小，分子为每个用户的Top-K推荐列表中属于测试集合的个数的总和。
在我们的实验中，由于我们只考虑用户购买序列的倒数第二个和最后一个商品作为验证集和测试集，故在给出的Top-K推荐列表中只有命中或未命中两种情况，故上式可简化为：

其中有两种取值0和1，为0表示用户的下一次购买的商品没有出现在Top-K推荐列表中，为1表示用户的下一次购买的商品出现在Top-K推荐列表中。
NDCG@10是一个加入了位置信息的评价指标，即真实购买的商品在推荐列表里的排名越高这个指数就越高，其形式化定义较为复杂，推导如下：
CG（Cumulative Gain，累积增益）表示用户对于Top-K推荐列表中每一个商品的累积评分。

其中表示用户对于Top-K推荐列表中在处于位置的商品的评分。
DCG（Discounted CG，折损累积增益）在CG的基础上考虑了推荐商品的顺序，对于推荐系统给出的排名越靠前的商品，如果用户评分越高，那么这个值越大。

IDCG（Ideal Discounted CG，理想折损累积增益）理想化的推荐列表，即推荐系统给出的商品列表中，刚好与用户评分排名最高的商品列表重合。

NDCG（Normalized Discounted CG，归一化折损累积增益）表示所有用户的DCG与IDCG之比的均值，我们可以用这个指标衡量推荐系统的效果。


在我们的实验中，与Hit Rate@10一样，我们只考虑用户购买序列的倒数第二个和最后一个商品作为验证集和测试集，故在给出的Top-K推荐列表中只有命中或未命中两种情况，故上式可简化为：




其中表示用户对于推荐列表中位置为的商品的评分，有两种取值0和1。
4.4 Baseline
为了展示我们的模型对效率的提升，我们比较了其他推荐算法的Baseline，结果是相比于其他模型我们的模型对于上述评价标准都有所提升。具体来说，我们的模型属于异质多模态的序列推荐算法，故我们比较了两组相关的模型的Baseline，包括序列推荐算法和多模态推荐算法。
对于序列推荐算法而言，我们比较了下列几个模型：
GRU4Rec[22]：一种基于RNN的模型，用GRU做基于会话的推荐。
Caser[21]：一种基于CNN的模型，用卷积核作用于邻近的序列，从商品ID序列中建模用户的兴趣。
SASRec[1]：一种基于attention的模型，采用纯attention的方式建模用户兴趣，从商品ID序列中自动学习商品之间的关联性特征，找出用户下一次购买的商品。
对于多模态推荐算法而言，我们比较了下列几个模型：
VBPR[25]：一种利用图像特征的BPR模型，BPR模型是一种经典的从用户反馈中学习个性化排名的算法，基于矩阵分解的思想。
JRL[32]：一种基于多源信息融合表征学习框架的推荐算法。
MV-RNN[26]：一种基于RNN的模型，利用了商品的图像和文本信息做推荐，属于多模态序列推荐算法。
4.5 损失函数和优化器
前面我们说过，为了使模型的输入为固定长度的向量序列，我们要对输入序列做填充或者截断，即给定，我们给出，我们定义t时刻的输出为：

我们的模型以为输入，以为输出。
对于模型的训练，对于每个时间戳的下一商品预测我们都选定了一个正样本和一个负样本，正样本即真实的下一商品，负样本是我们采用随机采样的方式从不在该用户购买序列中的商品中采样得到的商品，采用负样本可以减轻模型的训练负担。然后我们计算模型的输出和正负样本的点积作为分数。我们采用二元交叉熵损失函数作为我们的损失函数：

上式中我们忽略的项，其中为Sigmoid函数。
我们采用Adam优化器进行优化，该优化器是随机梯度下降（SGD）的变体，采用了动态适应的动量来进行评估，可以加速模型的收敛。
4.6 模型参数设置
对于除了我们的模型外的其他模型，他们的参数保持了原论文中的设置。对于SASRec和Caser，我们用了原论文作者开源的代码，对于剩下的模型，我们采用了Pytorch实现的模型。我们采用Pytorch实现了我们的模型。对于我们的模型，我们把Learning Rate设为0.001，Batch大小设置为128，Embedding大小为50，固定长度为100，Dropout Rate设为0.5。
4.7 与其他模型的比较
为了验证我们的模型对推荐的准确度是否有所提升，我们将我们的模型与一些近期相关的推荐模型的Baseline进行比较，结果如表5-2所示，表中为不同的模型在三个数据集上的表现，评价指标采用了Hit@10和NDCG@10，从结果可以看出我们的模型对比表现最佳的模型仍有提升。
表5-2 不同模型的表现
	Metric	Multimodal	Sequential	MSASR	impove%
		VBPR	JRL	MV-RNN	GRU4Rec	Caser	SASRec
Cell Phone & Accessories	Hit@10	27.78	34.27	53.49	44.14	49.73	58.93	59.67	0.74
	NDCG@10	15.62	19.91	33.23	27.03	31.72	38.66	39.03	0.37
Beauty	Hit@10	24.45	29.48	47.54	39.61	42.48	48.68	49.83	1.15
	NDCG@10	12.76	15.88	28.13	24.52	27.42	31.94	33.19	1.25
Clothing Shoes & Jewelry	Hit@10	17.53	22.3	34.22	27.55	28.27	37.63	38.28	0.65
	NDCG@10	8.69	11.89	19.96	15.57	16.19	21.98	22.42	0.44
从表中，我们可以有下列的观察：
对于序列推荐算法而言，即GRU4Rec、Caser和SASRec，他们都在这三个数据集上有着很好的表现，其中SASRec对比GRU4Rec和Caser都有更好的性能，一个可能的原因是，对于GRU4Rec和Caser而言，他们虽然都考虑了整个用户购买序列，但真实的情况可能是其中包含一些不相关的商品噪声，这些噪声同等地位地被算法学习了，即GRU4Rec和Caser不能独立地为序列中不同位置的商品的赋予不同的权重，一些不相关的商品被模型加以考虑了，导致指数偏低。而SASRec则采用了纯attention机制为不同的商品赋予权重，根据序列的不同位置考虑不同的商品对推荐结果的影响，为序列推荐提供了更加可靠和全面的信息，故取得了更好的效果。
对于多模态推荐而言，即VBPR、JRL和MV-RNN，他们的表现总体较序列推荐算法而言有所降低。VBPR和JRL属于纯多模态推荐算法，没有考虑序列信息，表现较序列推荐有大幅下降，其中VBPR仅考虑了视觉信息为用户做预测，效果不是很好，JRL设计了一个融合不同信息的推荐架构，获得了比VBPR更好的表现。MV-RNN是考虑了多模态信息的序列推荐算法，采用LSTM和GRU单元，融合了商品的图片和属性信息为用户做推荐，效果比单纯的多模态推荐算法好，但仍然比不上采用了attention机制且仅考虑了序列信息的SASRec。
比较序列推荐算法和多模态推荐算法，序列推荐算法在大多数情况下的表现都要优于多模态推荐算法，一个可能的原因是多模态推荐算法虽然考虑了商品的额外信息，如视觉和文本信息等，但对于推荐算法更重要的是商品序列的上下文信息，即推荐算法的准确性更依赖于用户的历史购买序列，因为其中包含了商品之间的关联性特征，这些隐藏的特征一般通过用户的购买序列来表现。
最后分析我们的模型，该模型在以上三个数据集上的表现对比其他模型都有所提升。一个可能的原因是，我们在SASRec的基础上对模型进行改进，SASRec本身就能够自适应地学习序列中不同位置的权重，本身就有良好的表现，更进一步，我们在序列中添加了额外的视觉和文本信息，克服了序列推荐算法仅靠商品ID序列学习商品关联性的问题，同时也解决了多模态推荐算法对于序列信息不敏感的问题。
4.8 剥离实验
4.8.1 对冷启动的问题的影响
为了验证该算法是否能缓解商品的冷启动问题，我们根据数据集中商品出现的频率把商品进行排序，接着把商品平均分为10组，即每一组代表了不同范围频率的商品集，然后我们分别用MSASR模型和SASRec模型做预测，结果如图5-1所示。实验结果可以看出，两种模型在低频率商品上的准确度都较高频率的商品有所降低，即存在商品的冷启动问题，但MSASR模型在低频率的商品上的表现优于SASRec模型，可以证明该模型在一定程度上缓解了商品的冷启动问题。

图 5-1 不同频率商品上的命中个数
4.8.2 不同隐层大小对实验结果的影响
隐层的大小属于模型中的超参数，我们比较了不同隐层大小对于模型的影响，并将其与SASRec模型进行对比，结果如图5-2和图5-3所示。实验结果可以看出，我们比较了隐层大小从10到50的Hit@10和NDCG@10，而较大的隐层的模型表现能力更好，可能的原因是更大的隐层可以更好地分辨信息，保留更多相关性，使模型能够学习到更抽象的特征。

图 5-3 不同隐层的Hit@10和NDCG@10折线图
4.8.3 不同模块对实验结果的影响
在我们的模型中，我们采用了许多不同的结构，不同的结构对模型有着不同的影响，接下来我们采用控制变量的方式分析不同的结构对模型结果产生的影响。我们分别比较了原始模型和它的4个变种模型，分别为移除了Positional Embedding，移除了Residual Connection，移除了Dropout以及采用多头的attention层，结果如表5-3所示。
表 5-3 改变不同模块的实验结果
	Hit@10	NDCG@10
Default	49.83	33.19
Remove PE	48.97	32.28
Remove RC	48.59	30.41
Remove Dropout	42.17	27.67
Multi-Head（2）	46.53	30.14
接下来我们分别分析改变的结构对模型产生的影响：
Remove PE（Positional Embedding）：Positional Embedding的作用是为序列中的不同位置的商品赋予位置信息，让模型计算权重时能够分辨序列中的近期商品和早期商品，从而有不同的偏向。当移除Positional Embedding时，attention层对序列中的每个商品同等看待，换句话说，模型就不考虑序列信息，只把商品序列看作是历史购买记录，从中学习用户偏好。一般来说，对于稠密数据集，即序列为某用户近期购买的商品序列时，Positional Embedding的影响较小，而对于长序列而言，Positional Embedding的影响较大。
Remove RC（Residual Connection）：Residual Connection的作用是把低层的信息直接传递到高层，增强模型的表达能力。当移除Residual Connection时，低层信息不能有效地传递到高层，从实验结果可以看出性能下降了，说明低层的信息对推荐有着影响。
Remove Dropout：Dropout通过训练时随机关闭某些单元来使模型有更好的表达能力，降低过拟合的风险，实现更好的性能。从实验结果可以看出，移除Dropout层后性能下降明显，可能导致了过拟合的问题。
Multi-Head（2）：Transformer论文[23]中提到的attention层采用的时多头的attention，即采用多组不同的矩阵分别得到多组Q，K，V，然后将结果的多个向量拼接并乘上另一个矩阵，使输出和输入是相同维度的向量，Transformer中的实验结果表明多头机制可以提高模型的表现，但在我们的实验中却出现了性能下降的情况，一个可能的原因是我们的隐层大小太小（50，Transformer中为512），不能很好地分解成更小的子空间。
4.9 本章小结
在这一章节，我们介绍了实验相关的一些工作以及实验的结果，从结果中可以看出我们的模型对推荐的准确度有所提升，且对于冷启动问题有一定的缓解



























第5章 总结与展望
5.1 论文工作总结
在这篇论文中，我们提出了一种异质多模态融合的基于注意力机制的推荐模型（Multimodal self-Attentive Sequential Recommendation，MSASR），能够及时学习到用户变化的兴趣，缓解商品冷启动和数据稀疏的问题。总的来说，首先，我们构建了商品序列的特征表示以及视觉文本的融合特征表示；接着，我们通过注意力机制从中学习到了用户兴趣：将商品特征序列通过self-attention模块，将融合特征序列通过另一个self-attention模块，再把两者的输出通过一个cross-attention模块，以商品序列为主，加入融合特征加强了模型的表达能力；最终，我们根据学习到的商品内在联系和用户兴趣为用户推荐商品。
5.2 后续工作展望
在目前的工作中，我们对于评论信息的处理较为简单：通过LDA分析把评论进行主题分类，该分析方法属于典型的词袋模型，可能存在词序导致的错误分析句意的问题。另外，我们对于图片和评论特征的融合也较为简单：我们把两者直接拼接，可能会为模型引入很多噪声，且评论的特征表示维度较低，导致模型对评论信息的关注可能不够。在未来的工作中，我们会致力于探索更好的对于评论信息的提取方式，并且找到融合多种模态的方法，构建更加高效的多模态序列推荐模型。















参考文献
[1] W. Kang and J. McAuley. self-Attentive Sequential Recommendation. ICDM, 2018.
[2] Md Mehrab Tanjim, Congzhe Su, Ethan Benjamin, Diane Hu, Liangjie Hong, and Julian McAuley. Attentive Sequential Models of Latent Intent for Next Item Recommendation. In Proceedings of The Web Conference 2020.
[3] Goldberg, David, et al. Using collaborative filtering to weave an information tapestry. Communications of the ACM 35.12 (1992): 61-71.
[4] Koren, Yehuda, Robert Bell, and Chris Volinsky. Matrix factorization techniques for recommender systems. Computer 8 (2009): 30-37.
[5] Rendle, Steffen. Factorization machines. 2010 IEEE International Conference on Data Mining. IEEE, 2010.
[6] Juan, Yuchin, et al. Field-aware factorization machines for CTR prediction. Proceedings of the 10th ACM Conference on Recommender Systems. ACM, 2016.
[7] He, Xinran, et al. Practical lessons from predicting clicks on ads at facebook. Proceedings of the Eighth International Workshop on Data Mining for Online Advertising. ACM, 2014.
[8] Gai, Kun, et al. Learning piece-wise linear models from large scale data for ad click prediction. arXiv prepring arXiv:1704.05194 (2017).
[9] Sedhain, Suvash, et al. Autorec: Autoencoders meet collaborative filtering. Processings of the 24th International Conference on World Wide Web. ACM, 2015.
[10] Shan, Ying, et al. Deep crossing: Web-scale modeling without manually crafted ombinatorial features. Proceedings of the 22nd ACM SIGKDD international.
[11] He, Xiangnan, et al. Neural collaborative filtering. Proceedings of the 26th international conference on world wide web. International World Wide Web Conferences Steering Committee, 2017.
[12] Qu, Yanru, et al. Product-based neural networks for user response prediction. 2016 IEEE 16th International Conference on Data Mining (ICDM). IEEE, 2016.
[13] Cheng, Heng-Tze, et al. Wide deep learning for recommender systems. Proceedings of the Ist workshop on deep learning for recommende systems. ACM, 2016.
[14] Wang, Ruoxi, et al. Deep cross network for ad click predictions, Proceedings of the ADKDD&apos;17. ACM, 2017.
[15] Zhang, Weinan, Tianming Du, and Jun Wang. Deep learning over multi-field categorical data. European conference on information retrieval. Springer, Cham, 2016.
[16] Guo, Huifeng,etal. DeepFM: factorization-ma- based neural network for CTR prediction. arXiv preprint arXiv: 1703.04247(2017).
[17] He, Xiangnan, Tat-Seng Chua. Neural factorization machines for sparse predictive analytics. Proceedings of the 40th International ACM SIGIR conference on Research and Development in Information Retrieval. ACM, 2017.
[18] Xiao, Jun, et al. attentional factorization machines: Learning the weight of feature interactions via attention networks, ar Xiv preprint arXiv: 1708.04617(2017).
[19] Zheng, Guanjie, et al. DRN: A deep reinforcement learning framework for news Recommender. Proceedings of the 2018 World Wide Web Conference. International World Wide Web Conferences Steering Committee, 2018.
[20] S. Rendle, C. Freudenthaler, and L. Schmidt-Thieme, “Factorizing personalized markov chains for next-basket recommendation,” in WWW, 2010.
[21] J. Tang and K. Wang, “Personalized top-n sequential recommendation via convolutional sequence embedding,” in WSDM, 2018.
[22] B. Hidasi, A. Karatzoglou, L. Baltrunas, and D. Tikk, “Session-based recommendations with recurrent neural networks,” in ICLR, 2016.
[23] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin, “attention is all you need,” in NIPS, 2017.
[24] Sun, F.; Liu, J.; Wu, J.; Pei, C.; Lin, X.; Ou, W.; and Jiang, P. 2019. BERT4Rec: Sequential Recommendation with Bidi rectional Encoder Representations from Transformer. In CIKM 2019, 1441–1450.
[25] He, R.; and McAuley, J. J. 2016. VBPR: Visual Bayesian Personalized Ranking from Implicit Feedback. In AAAI 2016, 144–150. AAAI Press.
[26] Chen, X.; Chen, H.; Xu, H.; Zhang, Y.; Cao, Y.; Qin, Z.; and Zha, H. 2019. Personalized Fashion Recommendation with Visual Explanations based on Multimodal attention Network: Towards Visually Explainable Recommendation. In SIGIR 2019, 765–774. ACM.
[27] Q. Cui, S. Wu, Q. Liu, W. Zhong and L. Wang. MV-RNN: A Multi-View Recurrent Neural Network for Sequential Recommendation. In IEEE Transactions on Knowledge and Data Engineering, vol. 32, no. 2, pp. 317-331, 1 Feb. 2020, doi: 10.1109/TKDE.2018.2881260.
[28] Cheng, Z.; Ding, Y.; Zhu, L.; and Kankanhalli, M. S. 2018. Aspect-Aware Latent Factor Model: Rating Prediction with Ratings and Reviews. In WWW 2018, 639–648. ACM.
[29] Li, C.; Niu, X.; Luo, X.; Chen, Z.; and Quan, C. 2019. A Review-Driven Neural Model for Sequential Recommendation. In IJCAI 2019, 2866–2872.
[30] Ups and downs: Modeling the visual evolution of fashion trends with one-class collaborative filtering R. He, J. McAuley WWW, 2016.
[31] Image-based recommendations on styles and substitutes J. McAuley, C. Targett, J. Shi, A. van den Hengel SIGIR, 2015.
[32] Zhang, Y.; Ai, Q.; Chen, X.; and Croft, W. B. 2017. Joint Representation Learning for Top-N Recommendation with Heterogeneous Information Sources. In CIKM 2017, 1449–1458. ACM.
致 谢
光阴似箭，转眼大学四年的时光即将逝去，在大学四年的学习中，我得到了老师、同学、和朋友的关怀，在学位论文即将完成之际，我要向给予过我帮助和鼓励的所有人表达我真诚的谢意。
首先，我要感谢我的指导老师牛少彰老师及其组内的老师对我毕业设计的指导帮助，感谢韩滕跃学姐，黄建辉学长，童小海学长对我毕业设计的指导和帮助，是他们的悉心指导才使得我的毕业论文能够顺利完成。
其次，我要感谢大学四年为我传道授业的老师们，是他们的生动讲解使我对这个学科充满了热情和信心。
最后，我要感谢我的家人们和朋友们，是他们的关怀和帮助才使我一路走到最后，完成我的学业。
毕业在即，在今后的生活学习工作中，我会继续秉承刻苦努力的态度，不懈努力，来报答支持和帮助过我的人。